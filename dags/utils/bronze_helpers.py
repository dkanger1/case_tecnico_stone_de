"""
Utilit√°rios compartilhados para DAGs de ingest√£o Bronze.
Centraliza l√≥gica comum e evita duplica√ß√£o de c√≥digo.
"""

import os
import pandas as pd
import duckdb
import json
from datetime import datetime
from typing import Dict, Optional, List, Any, Tuple
import logging

logger = logging.getLogger(__name__)

# Configura√ß√£o centralizada
DB_PATH = "/opt/airflow/data/meu_data_warehouse.db"


class BronzeIngestionError(Exception):
    """Exce√ß√£o customizada para erros de ingest√£o bronze."""
    pass


def validate_file_exists(file_path: str) -> None:
    """
    Valida se o arquivo existe e n√£o est√° vazio.
    
    Args:
        file_path: Caminho do arquivo a validar
        
    Raises:
        FileNotFoundError: Se o arquivo n√£o existir
        BronzeIngestionError: Se o arquivo estiver vazio
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Arquivo n√£o encontrado: {file_path}")
    
    if os.path.getsize(file_path) == 0:
        raise BronzeIngestionError(f"Arquivo est√° vazio: {file_path}")
    
    logger.info(f"‚úÖ Arquivo validado: {file_path} ({os.path.getsize(file_path)} bytes)")


def read_csv_with_validation(
    file_path: str,
    expected_columns: Optional[List[str]] = None,
    min_rows: int = 0,
    encoding: str = 'utf-8',
    **pandas_kwargs
) -> pd.DataFrame:
    """
    L√™ CSV com valida√ß√µes de qualidade de dados.
    
    Args:
        file_path: Caminho do arquivo CSV
        expected_columns: Lista de colunas esperadas (opcional)
        min_rows: N√∫mero m√≠nimo de linhas esperadas
        encoding: Encoding do arquivo
        **pandas_kwargs: Argumentos adicionais para pd.read_csv
        
    Returns:
        DataFrame com os dados lidos
        
    Raises:
        BronzeIngestionError: Se valida√ß√µes falharem
    """
    try:
        logger.info(f"üìñ Lendo arquivo: {file_path}")
        df = pd.read_csv(file_path, encoding=encoding, **pandas_kwargs)
        
        # Valida√ß√£o: arquivo n√£o vazio
        if len(df) == 0:
            raise BronzeIngestionError(f"Arquivo CSV est√° vazio: {file_path}")
        
        # Valida√ß√£o: n√∫mero m√≠nimo de linhas
        if len(df) < min_rows:
            raise BronzeIngestionError(
                f"Arquivo tem apenas {len(df)} linhas, m√≠nimo esperado: {min_rows}"
            )
        
        # Valida√ß√£o: colunas esperadas
        if expected_columns:
            missing_cols = set(expected_columns) - set(df.columns)
            if missing_cols:
                raise BronzeIngestionError(
                    f"Colunas esperadas n√£o encontradas: {missing_cols}. "
                    f"Colunas dispon√≠veis: {list(df.columns)}"
                )
        
        # Log de informa√ß√µes
        logger.info(f"‚úÖ CSV lido com sucesso: {len(df)} linhas, {len(df.columns)} colunas")
        logger.info(f"üìã Colunas: {', '.join(df.columns.tolist())}")
        
        # Estat√≠sticas b√°sicas
        null_counts = df.isnull().sum()
        if null_counts.sum() > 0:
            logger.warning(f"‚ö†Ô∏è Valores nulos encontrados:\n{null_counts[null_counts > 0]}")
        
        return df
        
    except pd.errors.EmptyDataError:
        raise BronzeIngestionError(f"Arquivo CSV est√° vazio ou corrompido: {file_path}")
    except pd.errors.ParserError as e:
        raise BronzeIngestionError(f"Erro ao parsear CSV: {str(e)}")
    except Exception as e:
        raise BronzeIngestionError(f"Erro inesperado ao ler CSV: {str(e)}")


def get_duckdb_connection(db_path: str = DB_PATH):
    """
    Cria conex√£o com DuckDB usando context manager.
    
    Args:
        db_path: Caminho do arquivo DuckDB
        
    Returns:
        Conex√£o DuckDB
    """
    if not os.path.exists(os.path.dirname(db_path)):
        os.makedirs(os.path.dirname(db_path), exist_ok=True)
        logger.info(f"üìÅ Diret√≥rio criado: {os.path.dirname(db_path)}")
    
    return duckdb.connect(db_path)


def create_bronze_table(
    df: pd.DataFrame,
    table_name: str,
    db_path: str = DB_PATH,
    if_exists: str = "replace",
    add_metadata: bool = True
) -> Dict[str, Any]:
    """
    Cria tabela bronze no DuckDB com metadados de ingest√£o.
    
    Args:
        df: DataFrame com os dados
        table_name: Nome da tabela a ser criada
        db_path: Caminho do arquivo DuckDB
        if_exists: Comportamento se tabela existir ('replace', 'append', 'fail')
        add_metadata: Se True, adiciona colunas de metadados
        
    Returns:
        Dicion√°rio com informa√ß√µes da ingest√£o
        
    Raises:
        BronzeIngestionError: Se houver erro na cria√ß√£o da tabela
    """
    try:
        # Adicionar metadados de ingest√£o
        if add_metadata:
            df = df.copy()
            df['_ingested_at'] = datetime.utcnow()
            df['_source_file'] = table_name  # Pode ser melhorado para passar o caminho real
        
        logger.info(f"üíæ Criando tabela bronze: {table_name}")
        
        con = get_duckdb_connection(db_path)
        try:
            # Verificar se tabela existe
            tables = con.execute("SHOW TABLES").fetchdf()
            table_exists = table_name in tables['name'].values if len(tables) > 0 else False
            
            if table_exists and if_exists == "fail":
                raise BronzeIngestionError(f"Tabela {table_name} j√° existe e if_exists='fail'")
            
            # Criar ou substituir tabela
            if if_exists == "replace":
                con.execute(f"CREATE OR REPLACE TABLE {table_name} AS SELECT * FROM df")
                logger.info(f"‚úÖ Tabela {table_name} criada/substitu√≠da")
            elif if_exists == "append":
                if table_exists:
                    con.execute(f"INSERT INTO {table_name} SELECT * FROM df")
                    logger.info(f"‚úÖ Dados inseridos na tabela {table_name}")
                else:
                    con.execute(f"CREATE TABLE {table_name} AS SELECT * FROM df")
                    logger.info(f"‚úÖ Tabela {table_name} criada")
            else:
                con.execute(f"CREATE TABLE {table_name} AS SELECT * FROM df")
                logger.info(f"‚úÖ Tabela {table_name} criada")
            
            # Verificar tabela criada
            count_result = con.execute(f"SELECT COUNT(*) as total FROM {table_name}").fetchone()
            total_rows = count_result[0] if count_result else 0
            
            # Obter schema
            schema_result = con.execute(f"DESCRIBE {table_name}").fetchdf()
            columns = schema_result['column_name'].tolist()
            
            result = {
                "status": "success",
                "table_name": table_name,
                "rows_ingested": len(df),
                "total_rows_in_table": int(total_rows),
                "columns": columns,
                "ingested_at": datetime.utcnow().isoformat(),
                "schema": schema_result.to_dict('records')
            }
            
            logger.info(f"üìä Ingest√£o conclu√≠da: {result['rows_ingested']} linhas -> {table_name}")
            logger.info(f"üìã Total na tabela: {result['total_rows_in_table']} linhas")
            
            return result
        finally:
            con.close()
            
    except Exception as e:
        error_msg = f"Erro ao criar tabela {table_name}: {str(e)}"
        logger.error(f"‚ùå {error_msg}")
        raise BronzeIngestionError(error_msg)


def validate_json_field(
    df: pd.DataFrame,
    json_column: str,
    log_rejected: bool = True
) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Any]]:
    """
    Valida campo JSON em um DataFrame e separa linhas v√°lidas e inv√°lidas.
    
    Args:
        df: DataFrame com os dados
        json_column: Nome da coluna que cont√©m JSON
        log_rejected: Se True, loga informa√ß√µes sobre linhas rejeitadas
        
    Returns:
        Tupla com (DataFrame v√°lido, DataFrame rejeitado, estat√≠sticas)
    """
    if json_column not in df.columns:
        raise BronzeIngestionError(
            f"Coluna JSON '{json_column}' n√£o encontrada. "
            f"Colunas dispon√≠veis: {list(df.columns)}"
        )
    
    logger.info(f"üîç Validando campo JSON: {json_column}")
    
    valid_rows = []
    rejected_rows = []
    rejected_indices = []
    
    for idx, row in df.iterrows():
        json_value = row[json_column]
        
        # Verificar se √© nulo
        if pd.isna(json_value):
            rejected_rows.append({
                'index': idx,
                'row_data': row.to_dict(),
                'reason': 'JSON field is NULL'
            })
            rejected_indices.append(idx)
            continue
        
        # Converter para string se necess√°rio
        json_str = str(json_value).strip()
        
        # Verificar se est√° vazio
        if not json_str or json_str == '':
            rejected_rows.append({
                'index': idx,
                'row_data': row.to_dict(),
                'reason': 'JSON field is empty'
            })
            rejected_indices.append(idx)
            continue
        
        # Tentar parsear JSON
        try:
            json.loads(json_str)
            valid_rows.append(idx)
        except json.JSONDecodeError as e:
            rejected_rows.append({
                'index': idx,
                'row_data': row.to_dict(),
                'reason': f'Invalid JSON: {str(e)}',
                'json_value': json_str[:200]  # Primeiros 200 caracteres para log
            })
            rejected_indices.append(idx)
        except Exception as e:
            rejected_rows.append({
                'index': idx,
                'row_data': row.to_dict(),
                'reason': f'Unexpected error: {str(e)}',
                'json_value': json_str[:200]
            })
            rejected_indices.append(idx)
    
    # Separar DataFrames
    df_valid = df.loc[valid_rows].copy() if valid_rows else pd.DataFrame()
    df_rejected = df.loc[rejected_indices].copy() if rejected_indices else pd.DataFrame()
    
    # Estat√≠sticas
    total_rows = len(df)
    valid_count = len(valid_rows)
    rejected_count = len(rejected_indices)
    
    stats = {
        'total_rows': total_rows,
        'valid_rows': valid_count,
        'rejected_rows': rejected_count,
        'rejection_rate': (rejected_count / total_rows * 100) if total_rows > 0 else 0
    }
    
    # Logging
    valid_rate = (valid_count / total_rows * 100) if total_rows > 0 else 0
    logger.info(f"üìä Valida√ß√£o JSON conclu√≠da:")
    logger.info(f"   ‚úÖ Linhas v√°lidas: {valid_count:,} ({valid_rate:.2f}% v√°lidas)")
    logger.info(f"   ‚ùå Linhas rejeitadas: {rejected_count:,} ({stats['rejection_rate']:.2f}% rejeitadas)")
    
    if rejected_count > 0 and log_rejected:
        logger.warning(f"‚ö†Ô∏è QUALITY GATE: {rejected_count} linhas rejeitadas devido a JSON inv√°lido")
        
        # Log detalhado das primeiras 10 linhas rejeitadas
        for i, rejected in enumerate(rejected_rows[:10]):
            logger.warning(
                f"   Rejeitada #{i+1} (linha {rejected['index']}): "
                f"{rejected['reason']}"
            )
            if 'json_value' in rejected:
                logger.warning(f"      JSON (primeiros 200 chars): {rejected['json_value']}")
        
        if rejected_count > 10:
            logger.warning(f"   ... e mais {rejected_count - 10} linhas rejeitadas")
    
    return df_valid, df_rejected, stats


def ingest_csv_to_bronze(
    file_path: str,
    table_name: str,
    db_path: str = DB_PATH,
    expected_columns: Optional[List[str]] = None,
    min_rows: int = 0,
    if_exists: str = "replace",
    add_metadata: bool = True,
    validate_json: Optional[str] = None,
    **csv_kwargs
) -> Dict[str, Any]:
    """
    Fun√ß√£o completa de ingest√£o CSV para bronze.
    Combina valida√ß√£o, leitura e cria√ß√£o de tabela.
    
    Args:
        file_path: Caminho do arquivo CSV
        table_name: Nome da tabela bronze
        db_path: Caminho do arquivo DuckDB
        expected_columns: Colunas esperadas (opcional)
        min_rows: N√∫mero m√≠nimo de linhas
        if_exists: Comportamento se tabela existir
        add_metadata: Adicionar colunas de metadados
        validate_json: Nome da coluna JSON para validar (opcional)
        **csv_kwargs: Argumentos adicionais para pd.read_csv
        
    Returns:
        Dicion√°rio com informa√ß√µes da ingest√£o
    """
    logger.info(f"üöÄ Iniciando ingest√£o bronze: {file_path} -> {table_name}")
    
    # 1. Validar arquivo
    validate_file_exists(file_path)
    
    # 2. Ler CSV com valida√ß√µes
    df = read_csv_with_validation(
        file_path,
        expected_columns=expected_columns,
        min_rows=min_rows,
        **csv_kwargs
    )
    
    # 3. Validar JSON se solicitado (QUALITY GATE)
    json_stats = None
    if validate_json:
        df_valid, df_rejected, json_stats = validate_json_field(
            df,
            json_column=validate_json,
            log_rejected=True
        )
        
        if len(df_valid) == 0:
            raise BronzeIngestionError(
                f"QUALITY GATE FALHOU: Todas as {len(df)} linhas foram rejeitadas "
                f"devido a JSON inv√°lido na coluna '{validate_json}'. "
                f"Nenhuma linha ser√° inserida no banco."
            )
        
        # Usar apenas linhas v√°lidas
        df = df_valid
        logger.info(f"‚úÖ QUALITY GATE: {len(df)} linhas v√°lidas ser√£o inseridas")
    
    # 4. Criar tabela bronze
    result = create_bronze_table(
        df,
        table_name,
        db_path=db_path,
        if_exists=if_exists,
        add_metadata=add_metadata
    )
    
    # Adicionar estat√≠sticas de valida√ß√£o JSON ao resultado
    if json_stats:
        result['json_validation'] = json_stats
        result['rejected_rows_count'] = json_stats['rejected_rows']
    
    logger.info(f"‚úÖ Ingest√£o bronze conclu√≠da com sucesso!")
    return result

